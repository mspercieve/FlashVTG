Epoch:   0%|                                                                                                                    | 0/150 [00:00<?, ?it/s]2025-03-24 15:40:25.839:INFO:__main__ - [Epoch 1]
Training Iteration:   0%|                                                                                                       | 0/113 [00:00<?, ?it/s]
Epoch:   0%|                                                                                                                    | 0/150 [00:00<?, ?it/s]
torch.Size([116, 64, 256])
torch.Size([116, 64, 256])
Traceback (most recent call last):
  File "FlashVTG_ms/train.py", line 452, in <module>
    best_ckpt_path, eval_split_name, eval_path, debug, opt = start_training()
  File "FlashVTG_ms/train.py", line 423, in start_training
    train(model, criterion, optimizer, lr_scheduler, train_dataset, eval_dataset, opt)
  File "FlashVTG_ms/train.py", line 140, in train
    model, criterion, train_loader, optimizer, opt, epoch_i, tb_writer
  File "FlashVTG_ms/train.py", line 56, in train_epoch
    outputs = model(**model_inputs, targets=targets)
  File "/home/mvpserver20/anaconda3/envs/cgdetr/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSD1/minseok/MR_HD/FlashVTG/FlashVTG_ms/model.py", line 184, in forward
    video_emb, video_msk, pos_embed, attn_weights, saliency_scores = self.transformer(src, ~mask, pos, video_length=video_length, saliency_proj1=self.saliency_proj1, saliency_proj2=self.saliency_proj2)
  File "/home/mvpserver20/anaconda3/envs/cgdetr/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSD1/minseok/MR_HD/FlashVTG/FlashVTG_ms/transformer.py", line 149, in forward
    vid_fuse , attn_weights = self.cross_encoder(src, src_key_padding_mask=mask, pos=pos_embed, video_length=video_length)
  File "/home/mvpserver20/anaconda3/envs/cgdetr/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSD1/minseok/MR_HD/FlashVTG/FlashVTG_ms/transformer.py", line 237, in forward
    **kwargs
  File "/home/mvpserver20/anaconda3/envs/cgdetr/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSD1/minseok/MR_HD/FlashVTG/FlashVTG_ms/transformer.py", line 185, in forward
    **kwargs
  File "/home/mvpserver20/anaconda3/envs/cgdetr/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSD1/minseok/MR_HD/FlashVTG/FlashVTG_ms/transformer.py", line 464, in forward
    return self.forward_post(src, src_mask, src_key_padding_mask, pos, dummy=dummy, **kwargs)
  File "/SSD1/minseok/MR_HD/FlashVTG/FlashVTG_ms/transformer.py", line 420, in forward_post
    pos_src = self.with_pos_embed(src, pos)
  File "/SSD1/minseok/MR_HD/FlashVTG/FlashVTG_ms/transformer.py", line 411, in with_pos_embed
    return tensor if pos is None else tensor + pos
RuntimeError: The size of tensor a (75) must match the size of tensor b (116) at non-singleton dimension 0
